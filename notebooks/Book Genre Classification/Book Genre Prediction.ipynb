{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judging a Book by its Title\n",
    "\n",
    "The purpose of this notebook is to investigate whether the books could be categorized based on the title alone, without prior knowledge of it's author. However, determining the genre of a book is a difficult task because book's title can be ambiguous and genre's can be overarching. \n",
    "\n",
    "### Dataset:\n",
    "\n",
    "The dataset for this project has been borrowed from Cornel University and can be accessed at https://github.com/uchidalab/book-dataset. The data contains book cover images, title, author, class and category as its columns. \n",
    "\n",
    "### Aim:\n",
    "\n",
    "Given any book's title, classify the category (genre) to which the book might belong to, without prior knowledge of it's Author or Genre. The notebook focuses on classifying 207,572 books into a total of 32 Genre's using simple Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Image</th>\n",
       "      <th>Image_link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Class</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761183272</td>\n",
       "      <td>0761183272.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61Y5cOdH...</td>\n",
       "      <td>Mom's Family Wall Calendar 2016</td>\n",
       "      <td>Sandra Boynton</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1623439671</td>\n",
       "      <td>1623439671.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61t-hrSw...</td>\n",
       "      <td>Doug the Pug 2016 Wall Calendar</td>\n",
       "      <td>Doug the Pug</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00O80WC6I</td>\n",
       "      <td>B00O80WC6I.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41X-KQqs...</td>\n",
       "      <td>Moleskine 2016 Weekly Notebook, 12M, Large, Bl...</td>\n",
       "      <td>Moleskine</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761182187</td>\n",
       "      <td>0761182187.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61j-4gxJ...</td>\n",
       "      <td>365 Cats Color Page-A-Day Calendar 2016</td>\n",
       "      <td>Workman Publishing</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1578052084</td>\n",
       "      <td>1578052084.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51Ry4Tsq...</td>\n",
       "      <td>Sierra Club Engagement Calendar 2016</td>\n",
       "      <td>Sierra Club</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id           Image  \\\n",
       "0   761183272  0761183272.jpg   \n",
       "1  1623439671  1623439671.jpg   \n",
       "2  B00O80WC6I  B00O80WC6I.jpg   \n",
       "3   761182187  0761182187.jpg   \n",
       "4  1578052084  1578052084.jpg   \n",
       "\n",
       "                                          Image_link  \\\n",
       "0  http://ecx.images-amazon.com/images/I/61Y5cOdH...   \n",
       "1  http://ecx.images-amazon.com/images/I/61t-hrSw...   \n",
       "2  http://ecx.images-amazon.com/images/I/41X-KQqs...   \n",
       "3  http://ecx.images-amazon.com/images/I/61j-4gxJ...   \n",
       "4  http://ecx.images-amazon.com/images/I/51Ry4Tsq...   \n",
       "\n",
       "                                               Title              Author  \\\n",
       "0                    Mom's Family Wall Calendar 2016      Sandra Boynton   \n",
       "1                    Doug the Pug 2016 Wall Calendar        Doug the Pug   \n",
       "2  Moleskine 2016 Weekly Notebook, 12M, Large, Bl...           Moleskine   \n",
       "3            365 Cats Color Page-A-Day Calendar 2016  Workman Publishing   \n",
       "4               Sierra Club Engagement Calendar 2016         Sierra Club   \n",
       "\n",
       "   Class      Genre  \n",
       "0      3  Calendars  \n",
       "1      3  Calendars  \n",
       "2      3  Calendars  \n",
       "3      3  Calendars  \n",
       "4      3  Calendars  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = pd.read_csv('book_list.csv', encoding = \"ISO-8859-1\", names = ['Id', 'Image', 'Image_link', 'Title', 'Author', 'Class', 'Genre'])\n",
    "\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Let's check if there are any missing entries in the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                0\n",
       "Image             0\n",
       "Image_link        0\n",
       "Title             0\n",
       "Author        14413\n",
       "Class             0\n",
       "Genre             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 14413 Null values in 'Author' column. For the current project, 'Title' and 'Genre' are the only columns of interest. But for the sake of having a clean dataset, replace Null values in 'Author' column with 'Anonymous'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['Author'] = books['Author'].fillna('Anonymous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis and Pre - Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Getting the Unique Genre's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Calendars', 'Comics & Graphic Novels', 'Test Preparation',\n",
       "       'Mystery, Thriller & Suspense', 'Science Fiction & Fantasy',\n",
       "       'Romance', 'Humor & Entertainment', 'Literature & Fiction',\n",
       "       'Gay & Lesbian', 'Engineering & Transportation',\n",
       "       'Cookbooks, Food & Wine', 'Crafts, Hobbies & Home',\n",
       "       'Arts & Photography', 'Education & Teaching',\n",
       "       'Parenting & Relationships', 'Self-Help', 'Computers & Technology',\n",
       "       'Medical Books', 'Science & Math', 'Health, Fitness & Dieting',\n",
       "       'Business & Money', 'Law', 'Biographies & Memoirs', 'History',\n",
       "       'Politics & Social Sciences', 'Reference',\n",
       "       'Christian Books & Bibles', 'Religion & Spirituality',\n",
       "       'Sports & Outdoors', 'Teen & Young Adult', \"Children's Books\",\n",
       "       'Travel'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['Genre'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Category(Genre) Encoding using Label encoder\n",
    "\n",
    "For most machine learning algorithms to achieve the state-of-the-art results, it is necessary to convert any categorical or textual data into numerical values. Sklearn's Label Encoder is used to convert the categorical column 'Genre' into numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Encoding: [ 3  3  3 ... 31 31 31]\n",
      "Decoding: ['Calendars' 'Calendars' 'Calendars' ... 'Travel' 'Travel' 'Travel']\n",
      "\n",
      "Encoded and Decoded columns shape\n",
      "(207572,) (207572,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "genre_cat = le.fit_transform(books['Genre'])\n",
    "print('Categorical Encoding: {}'.format(genre_cat))\n",
    "\n",
    "genre = le.inverse_transform(genre_cat)\n",
    "print('Decoding: {}'.format(genre))\n",
    "\n",
    "print('\\nEncoded and Decoded columns shape')\n",
    "print(genre.shape, genre_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Filtering useless data from Book's Title\n",
    "\n",
    "Stopwords are the commonly used words (basic typical english words) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. Since these words take up a lot of space and processing time, it must be filtered out! This can be done using NLTK(Natural Language Toolkit). \n",
    "\n",
    "    >>> import nltk\n",
    "    >>> nltk.download('stopwords')\n",
    "\n",
    "For better results, let's filter the 'Title' column and remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of stopwords: 179 \n",
      "First few stopwords: ['i', 'me', 'my', 'myself', 'we']\n",
      "\n",
      "\n",
      "0                           Mom's Family Wall Calendar 2016\n",
      "1                           Doug the Pug 2016 Wall Calendar\n",
      "2         Moleskine 2016 Weekly Notebook, 12M, Large, Bl...\n",
      "3                   365 Cats Color Page-A-Day Calendar 2016\n",
      "4                      Sierra Club Engagement Calendar 2016\n",
      "                                ...                        \n",
      "207567    ADC the Map People Washington D.C.: Street Map...\n",
      "207568    Washington, D.C., Then and Now: 69 Sites Photo...\n",
      "207569    The Unofficial Guide to Washington, D.C. (Unof...\n",
      "207570        Washington, D.C. For Dummies (Dummies Travel)\n",
      "207571    Fodor's Where to Weekend Around Boston, 1st Ed...\n",
      "Name: title_alt, Length: 207572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = list(stopwords.words('english'))\n",
    "print('Total Number of stopwords: {} '.format(len(stop))) \n",
    "\n",
    "print('First few stopwords: {}'.format(stop[:5])) # typical english words to be removed from the title!\n",
    "\n",
    "def change(t):\n",
    "    # This function takes in each 'Title' and removes the stopwords from it\n",
    "    return ' '.join([(i) for (i) in t if i not in stop])\n",
    "\n",
    "books['title_alt'] = pd.DataFrame(books['Title'])\n",
    "books['title_alt'].apply(change)\n",
    "\n",
    "print('\\n')\n",
    "print(books['title_alt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Feature Extraction with TF-IDF\n",
    "\n",
    "TF-IDF stands for Term Frequency – Inverse Document Frequency. It is one of the most important techniques used for information retrieval to represent how important a specific word or phrase is to a given document. Given an abundant textual feature (like title of a book), TF-IDF converts raw strings into vectors and each word has its own vector. Useful features from this vectors is chosen based on the numeric value assigned by Tf-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207572, 50000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 50000, strip_accents = 'unicode', lowercase = True, analyzer = 'word', token_pattern = r'\\w+', use_idf = True, smooth_idf = True, sublinear_tf = True, stop_words = 'english')\n",
    "\n",
    "vectors = vectorizer.fit_transform(books['title_alt'])\n",
    "vectors.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Split the dataset into Training and Testing sets\n",
    "\n",
    "Now that the feature and target are pre-processed and ready, split them into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: (166057, 50000)\n",
      "Testing samples: (41515, 50000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, genre_cat, test_size = 0.2)\n",
    "\n",
    "print('Training samples: {}'.format(X_train.shape, y_train.shape))\n",
    "print('Testing samples: {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Evaluation\n",
    "\n",
    "Let's use the typical method for any classifying problems ~ Logistic Regression. Fit the logistic regressor using the training sets and evaluating the model's accuracy using testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver = 'sag', max_iter = 200)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 5  7 23 ... 31  2 26]\n",
      "F1 Score: 0.581741\n",
      "Accuracy: 0.628616\n"
     ]
    }
   ],
   "source": [
    "# Prediction and Evaluation\n",
    "\n",
    "hyp = lr.predict(X_test)\n",
    "print('Predictions: {}'.format(hyp))\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print ('F1 Score: {:2f}'.format(metrics.f1_score(y_test, hyp, average = 'macro')))\n",
    "print ('Accuracy: {:2f}'.format(metrics.accuracy_score(y_test, hyp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is pretty good in predicting the Genre from the book's title, considering the Book's title can be ambiguous or a single book could belong to multiple Genre's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supriya/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'bookGenrePrediction_Model.pkl')\n",
    "print (\"Model Saved...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model on User Inputs\n",
    "\n",
    "It's time to reload the model and check whether it predicts correctly for my usecases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reloading Model\n",
    "\n",
    "model = joblib.load('bookGenrePrediction_Model.pkl')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Travel', 'Comics & Graphic Novels', \"Children's Books\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My Use-cases\n",
    "\n",
    "my_books = ['Lonely Planet Iceland', 'Avatar', 'Famous Five']\n",
    "vt = (vectorizer.transform(my_books))\n",
    "\n",
    "pred = (model.predict(vt)) # Predict Genre's for my books!\n",
    "\n",
    "le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! It's a perfect prediction!\n",
    "##### That's All !! Happy Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
